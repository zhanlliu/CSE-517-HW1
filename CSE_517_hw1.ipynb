{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 517 hw1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aJEOnodOyOH-",
        "colab_type": "code",
        "outputId": "0bb0e711-fb8a-4831-c4bf-fb05ab883147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#### Import packages and set working directory\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/cse517/hw1')\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-pUDHiGBzEtP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Import data \n",
        "\n",
        "train = pd.read_csv('brown.train.txt',delimiter='\\t',encoding='utf-8', header = None)\n",
        "valid = pd.read_csv('brown.dev.txt',delimiter='\\t',encoding='utf-8', header = None)\n",
        "test = pd.read_csv('brown.test.txt',delimiter='\\t',encoding='utf-8', header = None)\n",
        "## Extract only 50% of the entire dataset for exploring purpose\n",
        "#random.seed(517)\n",
        "#train = train.sample(frac=0.5, replace =False)\n",
        "train.columns =['text']\n",
        "valid.columns =['text']\n",
        "test.columns =['text']\n",
        "\n",
        "#### Adding start symbols and end symbols\n",
        "SENTENCE_START_1 = \"<s>\"\n",
        "SENTENCE_START_2 = \"<ss>\"\n",
        "SENTENCE_END = \"</s>\"\n",
        "\n",
        "for i in train.index:\n",
        "  train['text'][i] = SENTENCE_START_1 + ' ' + SENTENCE_START_2 + ' ' + train['text'][i] + ' ' + SENTENCE_END\n",
        "  \n",
        "for i in valid.index:\n",
        "  valid['text'][i] = SENTENCE_START_1 + ' ' + SENTENCE_START_2 + ' ' + valid['text'][i] + ' ' + SENTENCE_END\n",
        "  \n",
        "for i in test.index:\n",
        "  test['text'][i] = SENTENCE_START_1 + ' ' + SENTENCE_START_2 + ' ' + test['text'][i] + ' ' + SENTENCE_END"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyuGhbNVzmTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Finding low frequent words in the training set and replace them as '<unk>'\n",
        "train = pd.DataFrame({'text' : train['text']})\n",
        "train_cat = train['text'].str.cat(sep=' \\n ')\n",
        "#### low_freq can be altered \n",
        "low_freq = 1\n",
        "from collections import Counter\n",
        "counts = Counter(train_cat.split(' '))\n",
        "train_cat_replaced = ' '.join(i if counts[i] > low_freq else '<unk>' for i in train_cat.split(' '))\n",
        "train_text = train_cat_replaced.split(' \\n ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCTx6kd0z0At",
        "colab_type": "code",
        "outputId": "133557b7-a80c-441e-e5ed-95846c8d91a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "######################## Unigram #############################\n",
        "##############################################################\n",
        "#### Build unigram dictionary for the training text\n",
        "unigram_frequencies = dict()\n",
        "corpus_length = 0\n",
        "for sentence in train_text:\n",
        "  for word in sentence.split():\n",
        "      unigram_frequencies[word] = unigram_frequencies.get(word, 0) + 1\n",
        "      if word != SENTENCE_START_1 and word != SENTENCE_START_2:\n",
        "          corpus_length += 1\n",
        "unigram_frequencies['\\n'] = 0\n",
        "\n",
        "#### Handle the out of vocabuary case for the dev set and test set\n",
        "valid = pd.DataFrame({'text' : valid['text']})\n",
        "valid_cat = valid['text'].str.cat(sep=' \\n ')\n",
        "valid_cat_replaced = ' '.join(i if i in unigram_frequencies.keys()  else '<unk>' for i in valid_cat.split(' '))\n",
        "valid_text = valid_cat_replaced.split(' \\n ')\n",
        "\n",
        "test = pd.DataFrame({'text' : test['text']})\n",
        "test_cat = test['text'].str.cat(sep=' \\n ')\n",
        "test_cat_replaced = ' '.join(i if i in unigram_frequencies.keys()  else '<unk>' for i in test_cat.split(' '))\n",
        "test_text = test_cat_replaced.split(' \\n ')\n",
        "\n",
        "unigram_frequencies.pop('\\n', None)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "3zj4ov3i0ibF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Calculate unigram probabilty for a specific word\n",
        "def calculate_unigram_probability(word):\n",
        "    word_probability_numerator = unigram_frequencies.get(word, 0)\n",
        "    word_probability_denominator = corpus_length\n",
        "    return float(word_probability_numerator) / float(word_probability_denominator)\n",
        "  \n",
        "#### Calculate a specific sentence probabilty by producting the probabilty for each word  \n",
        "def calculate_sentence_probability(sentence):\n",
        "    sentence_probability_log_sum = 0\n",
        "    for word in sentence.split():\n",
        "        if word != SENTENCE_START_1 and word != SENTENCE_START_2:\n",
        "          word_probability = calculate_unigram_probability(word)\n",
        "          sentence_probability_log_sum += math.log(word_probability, 2)\n",
        "    return(sentence_probability_log_sum) \n",
        "  \n",
        "#### Calculate the corpus for the data set \n",
        "def calculate_corpus(data):\n",
        "  corpus = 0 \n",
        "  for sentence in data:\n",
        "    for word in sentence.split():\n",
        "        if word != SENTENCE_START_1 and word != SENTENCE_START_2:\n",
        "            corpus += 1\n",
        "  return(corpus)\n",
        "# sum(unigram_frequencies.values()) - unigram_frequencies['<s>'] - unigram_frequencies['<ss>']\n",
        "\n",
        "#### Calculate the perplexity for the data set using the unigram model\n",
        "def calculate_perplexity(data):\n",
        "  log_loss = 0\n",
        "  corpus = calculate_corpus(data)\n",
        "  for sentence in data:\n",
        "    log_loss += calculate_sentence_probability(sentence)\n",
        "  log_loss = log_loss/corpus\n",
        "  return(math.pow(2, -log_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ekR5wL1u1ZvW",
        "colab_type": "code",
        "outputId": "67ee0a38-1bd4-46a6-fefd-bace7232b5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Unigram Model\")\n",
        "print(\"The perplexity for the training set is %f\"%(calculate_perplexity(train_text)))\n",
        "print(\"The perplexity for the dev set is %f\"%(calculate_perplexity(valid_text)))\n",
        "print(\"The perplexity for the test set is %f\"%(calculate_perplexity(test_text)))\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram Model\n",
            "The perplexity for the training set is 868.099627\n",
            "The perplexity for the dev set is 765.903877\n",
            "The perplexity for the test set is 769.370394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t1qD6VYa1r-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "######################## Bigram #############################\n",
        "##############################################################\n",
        "#### Build bigram dictionary for the training text\n",
        "bigram_frequencies = dict()\n",
        "unique_bigrams = set()\n",
        "corpus_length = 0\n",
        "for sentence in train_text:\n",
        "  previous_word = None\n",
        "  for word in sentence.split():\n",
        "    if previous_word != None and previous_word != \"</s>\":\n",
        "      bigram_frequencies[(previous_word, word)] = bigram_frequencies.get((previous_word, word), 0 )+1\n",
        "      #unigram_frequencies[word] = unigram_frequencies.get(word, 0) + 1\n",
        "      if previous_word != SENTENCE_START_2 and word != SENTENCE_END:\n",
        "          unique_bigrams.add((previous_word, word))\n",
        "    previous_word = word \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mi8Oh4uu2cum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Calculate bigram probabilty for a specific bigram word\n",
        "def calculate_bigram_probability(previous_word, word):\n",
        "  bigram_word_numerator = bigram_frequencies.get((previous_word, word),0)\n",
        "  bigram_word_denominator = unigram_frequencies.get(previous_word)\n",
        "  return(bigram_word_numerator/bigram_word_denominator)\n",
        "\n",
        "#calculate_bigram_probability('an', '<unk>')\n",
        "\n",
        "#### Calculate a specific sentence probabilty by using the bigram probability\n",
        "def calculate_bigram_sentence_probability(sentence):\n",
        "  bigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  for word in sentence.split():\n",
        "    #print(previous_word, word)\n",
        "    if previous_word !=None:\n",
        "      bigram_word_probability = calculate_bigram_probability(previous_word, word)\n",
        "      if bigram_word_probability ==0.0:\n",
        "        return(-math.inf)\n",
        "      else:\n",
        "        bigram_sentence_probability_log_sum += math.log(bigram_word_probability, 2)\n",
        "    previous_word = word\n",
        "  return(bigram_sentence_probability_log_sum)\n",
        "\n",
        "#### Calculate the perplexity for the data set using the bigram model\n",
        "def bigram_calculate_perplexity(data):\n",
        "  bigram_log_loss = 0\n",
        "  corpus = calculate_corpus(data)\n",
        "  for sentence in data:\n",
        "    bigram_log_loss += calculate_bigram_sentence_probability(sentence)\n",
        "  bigram_log_loss = bigram_log_loss/corpus\n",
        "  return(math.pow(2, -bigram_log_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "REvU60Se3LxX",
        "colab_type": "code",
        "outputId": "19a79826-6d7e-4254-85fd-b9b0f9cf326b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Bigram Model\")\n",
        "print(\"The perplexity for the training set is %f\"%(bigram_calculate_perplexity(train_text)))\n",
        "print(\"The perplexity for the dev set is %f\"%(bigram_calculate_perplexity(valid_text)))\n",
        "print(\"The perplexity for the test set is %f\"%(bigram_calculate_perplexity(test_text)))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigram Model\n",
            "The perplexity for the training set is 65.018176\n",
            "The perplexity for the dev set is inf\n",
            "The perplexity for the test set is inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NWnjt9c63Zmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "######################## Trigram(Section II) #########################\n",
        "######################################################################\n",
        "#### Build trigram dictionary for the training set\n",
        "trigram_frequencies = dict()\n",
        "unique_trigrams = set()\n",
        "#corpus_length = 0\n",
        "for sentence in train_text:\n",
        "  word_1 = None\n",
        "  word_2 = None\n",
        "  for word in sentence.split():\n",
        "    if word_1 != None and word_2 != None and word_2 != \"</s>\":\n",
        "      trigram_frequencies[(word_1, word_2, word)] = trigram_frequencies.get((word_1, word_2, word), 0 ) + 1\n",
        "      if word_1 != SENTENCE_START_1 and word_2 != SENTENCE_START_2 and word != SENTENCE_END:\n",
        "          unique_trigrams.add((word_1, word_2, word))\n",
        "    word_1 = word_2\n",
        "    word_2 = word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxX4iUKU34c-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Calculate trigram probabilty for a specific trigram word\n",
        "def calculate_trigram_probability(word_1, word_2, word):\n",
        "  trigram_word_numerator = trigram_frequencies.get((word_1, word_2, word),0)\n",
        "  trigram_word_denominator = bigram_frequencies.get((word_1, word_2),0)\n",
        "  return(trigram_word_numerator/trigram_word_denominator)\n",
        "\n",
        "#### Calculate a specific sentence probabilty by using the trigram probability\n",
        "def calculate_trigram_sentence_probability(sentence):\n",
        "  trigram_sentence_probability_log_sum = 0\n",
        "  word_1 = None\n",
        "  word_2 = None\n",
        "  for word in sentence.split():\n",
        "    #print(previous_word, word)\n",
        "    if word_1 !=None and word_2 != None:\n",
        "      trigram_word_probability = calculate_trigram_probability(word_1, word_2, word)\n",
        "      if trigram_word_probability ==0.0:\n",
        "        return(-math.inf)\n",
        "      else:\n",
        "        trigram_sentence_probability_log_sum += math.log(trigram_word_probability, 2)\n",
        "    word_1 = word_2\n",
        "    word_2 = word\n",
        "  return(trigram_sentence_probability_log_sum)\n",
        "\n",
        "#### Calculate the perplexity for the data set using the trigram model\n",
        "def trigram_calculate_perplexity(data):\n",
        "  trigram_log_loss = 0\n",
        "  corpus = calculate_corpus(data)\n",
        "  for sentence in data:\n",
        "    trigram_log_loss += calculate_trigram_sentence_probability(sentence)\n",
        "  trigram_log_loss = trigram_log_loss/corpus\n",
        "  return(math.pow(2, -trigram_log_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4DmHSuWq4NZH",
        "colab_type": "code",
        "outputId": "84b76d62-95f2-41c6-e111-ef1984a2ddb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Trigram Model\")\n",
        "print(\"The perplexity for the training set is %f\"%(trigram_calculate_perplexity(train_text)))\n",
        "print(\"The perplexity for the dev set is %f\"%(trigram_calculate_perplexity(valid_text)))\n",
        "print(\"The perplexity for the test set is %f\"%(trigram_calculate_perplexity(test_text)))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigram Model\n",
            "The perplexity for the training set is 7.027090\n",
            "The perplexity for the dev set is inf\n",
            "The perplexity for the test set is inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sXzGcgBC4pRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######################################################################################\n",
        "######################## Trigram(Section III) add K smoothing #########################\n",
        "#######################################################################################\n",
        "unique_unigram_length = len(unigram_frequencies.keys()) -2\n",
        "unique_bigram_length = len(bigram_frequencies.keys()) -1\n",
        "unique_trigram_length = len(trigram_frequencies.keys()) \n",
        "\n",
        "#### Add K trigram probablity \n",
        "def calculate_trigram_probability_addK(word_1, word_2, word, K):\n",
        "    trigram_word_numerator = trigram_frequencies.get((word_1, word_2, word),0) + K\n",
        "    trigram_word_denominator = bigram_frequencies.get((word_1, word_2),0) + K*unique_unigram_length\n",
        "    return(trigram_word_numerator/trigram_word_denominator)\n",
        "\n",
        "##### Sentence probability with add K smoothing \n",
        "def calculate_trigram_sentence_probability_addK(sentence, K):\n",
        "  trigram_sentence_probability_log_sum = 0\n",
        "  word_1 = None\n",
        "  word_2 = None\n",
        "  for word in sentence.split():\n",
        "    #print(previous_word, word)\n",
        "    if word_1 !=None and word_2 != None:\n",
        "      trigram_word_probability = calculate_trigram_probability_addK(word_1, word_2, word, K)\n",
        "      trigram_sentence_probability_log_sum += math.log(trigram_word_probability, 2)\n",
        "    word_1 = word_2\n",
        "    word_2 = word\n",
        "  return(trigram_sentence_probability_log_sum)\n",
        "\n",
        "#### Trigram perplexity with add K smoothing  \n",
        "def trigram_calculate_perplexity_addK(data, K):\n",
        "  trigram_log_loss = 0\n",
        "  corpus = calculate_corpus(data)\n",
        "  for sentence in data:\n",
        "    trigram_log_loss += calculate_trigram_sentence_probability_addK(sentence, K)\n",
        "  trigram_log_loss = trigram_log_loss/corpus\n",
        "  return(math.pow(2, -trigram_log_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQmvUJei5saX",
        "colab_type": "code",
        "outputId": "1bd6502f-a118-4c91-b262-a6008edfd708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "K_list = [0.001, 0.01, 0.1, 1]\n",
        "res_k = np.zeros((len(K_list), 3))\n",
        "for k in range(len(K_list)):\n",
        "  print(\"Add %f smoothing trigram model\" %(K_list[k]))\n",
        "  print(\"training set, Perplexity:%f\" %( trigram_calculate_perplexity_addK(train_text, K_list[k])))\n",
        "  print(\"develop set, Perplexity:%f\" %( trigram_calculate_perplexity_addK(valid_text, K_list[k])))\n",
        "  #print(\"test set, Perplexity:%f\" %( trigram_calculate_perplexity(test_text, K_list[k])))\n",
        "  res_k[k, 0] = trigram_calculate_perplexity_addK(train_text, K_list[k])\n",
        "  res_k[k, 1] = trigram_calculate_perplexity_addK(valid_text, K_list[k])\n",
        "  res_k[k, 2] = trigram_calculate_perplexity_addK(test_text, K_list[k])\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Add 0.001000 smoothing trigram model\n",
            "training set, Perplexity:35.974900\n",
            "develop set, Perplexity:2832.742572\n",
            "Add 0.010000 smoothing trigram model\n",
            "training set, Perplexity:183.351504\n",
            "develop set, Perplexity:3421.921512\n",
            "Add 0.100000 smoothing trigram model\n",
            "training set, Perplexity:1224.627900\n",
            "develop set, Perplexity:5654.727267\n",
            "Add 1.000000 smoothing trigram model\n",
            "training set, Perplexity:6769.136077\n",
            "develop set, Perplexity:10578.830463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ucrIRqMiqZ8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a6f4d5d-0f4a-454e-b994-2cbaca1e1a88"
      },
      "cell_type": "code",
      "source": [
        "unigram_corpus"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1064606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "metadata": {
        "id": "MJUKRh6j5vyP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "######################## Trigram(Section III) Linear Interpolation #########################\n",
        "############################################################################################\n",
        "\n",
        "def calculate_trigram_probability_LP(word_1, word_2, word):\n",
        "  if bigram_frequencies.get((word_1, word_2), 0) ==0:\n",
        "    return(1/unique_unigram_length)\n",
        "  else:\n",
        "    trigram_word_numerator = trigram_frequencies.get((word_1, word_2, word), 0) \n",
        "    trigram_word_denominator = bigram_frequencies.get((word_1, word_2)) \n",
        "    return(trigram_word_numerator/trigram_word_denominator)\n",
        "\n",
        "def calculate_bigram_probability_LP(word_2, word):\n",
        "  if unigram_frequencies.get(word_2, 0) ==0:\n",
        "    return(1/unique_unigram_length)\n",
        "  else:\n",
        "    bigram_word_numerator = bigram_frequencies.get((word_2,word),0)\n",
        "    bigram_word_denominator = unigram_frequencies.get(word_2)\n",
        "    return(bigram_word_numerator/bigram_word_denominator)\n",
        "\n",
        "#unigram_corpus = sum(unigram_frequencies.values())\n",
        "def calculate_unigram_probability_LP(word):\n",
        "  unigram_word_numerator = unigram_frequencies.get(word)\n",
        "  unigram_word_denominator = unigram_corpus \n",
        "  return(unigram_word_numerator/unigram_word_denominator)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_interpolation_probability(word_1, word_2, word):\n",
        "  tri_prob = calculate_trigram_probability_LP(word_1, word_2, word)\n",
        "  bi_prob = calculate_bigram_probability_LP(word_2, word)\n",
        "  uni_prob = calculate_unigram_probability_LP(word)\n",
        "  res = lambda_1 * tri_prob + lambda_2 * bi_prob + lambda_3 * uni_prob\n",
        "  return(res)\n",
        "\n",
        "def calculate_interpolation_sentence_probability(sentence):\n",
        "  interpolation_sentence_probability_log_sum = 0\n",
        "  word_1 = None\n",
        "  word_2 = None\n",
        "  for word in sentence.split():\n",
        "    #print(previous_word, word)\n",
        "    if word_1 !=None and word_2 != None and word_2 !=\"</s>\":\n",
        "      interpolation_word_probability = calculate_interpolation_probability(word_1, word_2, word)\n",
        "      interpolation_sentence_probability_log_sum += math.log(interpolation_word_probability, 2)\n",
        "    word_1 = word_2\n",
        "    word_2 = word\n",
        "  return(interpolation_sentence_probability_log_sum)\n",
        "\n",
        "\n",
        "def interpolation_calculate_perplexity(data):\n",
        "  interpolation_log_loss = 0\n",
        "  corpus = calculate_corpus(data)\n",
        "  for sentence in data:\n",
        "    interpolation_log_loss += calculate_interpolation_sentence_probability(sentence)\n",
        "  interpolation_log_loss = interpolation_log_loss/corpus\n",
        "  return(math.pow(2, -interpolation_log_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpDznXf_5xKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lambda_list = [[0.1, 0.6, 0.3], [0.1, 0.3, 0.6], [0.3, 0.3, 0.4], [0.5, 0.3, 0.2], [0.7, 0.2,0.1]]\n",
        "res_lambda = np.zeros((len(lambda_list), 3))\n",
        "for i in range(len(lambda_list)):\n",
        "  #print(\"lambda_1: %f, lambda_2: %f, lambda_3: %f \" %(lambda_1, lambda_2, lambda_3) )\n",
        "  lambda_1 = lambda_list[i][0]\n",
        "  lambda_2 = lambda_list[i][1]\n",
        "  lambda_3 = lambda_list[i][2]\n",
        "  res_lambda[i][0] = interpolation_calculate_perplexity(train_text)\n",
        "  res_lambda[i][1] = interpolation_calculate_perplexity(valid_text)\n",
        "  res_lambda[i][2] = interpolation_calculate_perplexity(test_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nXV8S5XaFura",
        "colab_type": "code",
        "outputId": "9fcbbecf-daa8-42da-adcf-13dcea73c6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "res_lambda"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 27.27246554, 266.29232132, 267.70988131],\n",
              "       [ 34.12926476, 296.94173709, 299.10077405],\n",
              "       [ 16.86568304, 281.69135512, 283.84840571],\n",
              "       [ 11.46721632, 305.1541135 , 307.47101882],\n",
              "       [  9.03785219, 386.60163337, 389.79324727]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "metadata": {
        "id": "wrNrJj1GHGQi",
        "colab_type": "code",
        "outputId": "0a54058e-2fb9-4465-9742-68973c3b97f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "unique_unigram_length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "metadata": {
        "id": "y9IJEiKxR3fk",
        "colab_type": "code",
        "outputId": "9f01ae5a-9888-4f29-baf4-f581b736408c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_corpus(train_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "972962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "metadata": {
        "id": "hNMoVnvzT5zF",
        "colab_type": "code",
        "outputId": "f269c889-90c2-4cd9-d208-b7325c2cc5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_corpus(valid_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123380"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "metadata": {
        "id": "7dxRTwH9T76l",
        "colab_type": "code",
        "outputId": "4095a4ad-7f1c-46c4-d3c7-ad871ae2219a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_corpus(test_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122190"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "metadata": {
        "id": "PPBN5IhfUACN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}